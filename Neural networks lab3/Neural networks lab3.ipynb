{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, LSTM, InputLayer\n",
    "from tensorflow.keras import regularizers\n",
    "#from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Максимальное количество слов \n",
    "num_words = 10000\n",
    "# Максимальная длина новости\n",
    "max_news_len = 30\n",
    "# Количество классов новостей\n",
    "nb_classes = 4\n",
    "\n",
    "train = pd.read_csv('train.csv', header=None, names=['class', 'title', 'text'])\n",
    "news = train['text']\n",
    "y_train = utils.to_categorical(train['class'] - 1, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'a': 2,\n",
       " 'to': 3,\n",
       " 'of': 4,\n",
       " 'in': 5,\n",
       " 'and': 6,\n",
       " 'on': 7,\n",
       " 'for': 8,\n",
       " '39': 9,\n",
       " 's': 10,\n",
       " 'that': 11,\n",
       " 'with': 12,\n",
       " 'as': 13,\n",
       " 'its': 14,\n",
       " 'at': 15,\n",
       " 'said': 16,\n",
       " 'is': 17,\n",
       " 'by': 18,\n",
       " 'it': 19,\n",
       " 'has': 20,\n",
       " 'new': 21,\n",
       " 'an': 22,\n",
       " 'from': 23,\n",
       " 'reuters': 24,\n",
       " 'his': 25,\n",
       " 'will': 26,\n",
       " 'was': 27,\n",
       " 'after': 28,\n",
       " 'have': 29,\n",
       " 'be': 30,\n",
       " 'their': 31,\n",
       " 'two': 32,\n",
       " 'are': 33,\n",
       " 'us': 34,\n",
       " 'over': 35,\n",
       " 'quot': 36,\n",
       " 'year': 37,\n",
       " 'first': 38,\n",
       " 'ap': 39,\n",
       " 'he': 40,\n",
       " 'but': 41,\n",
       " 'gt': 42,\n",
       " 'lt': 43,\n",
       " 'this': 44,\n",
       " 'more': 45,\n",
       " 'monday': 46,\n",
       " 'wednesday': 47,\n",
       " 'one': 48,\n",
       " 'tuesday': 49,\n",
       " 'up': 50,\n",
       " 'thursday': 51,\n",
       " 'company': 52,\n",
       " 'inc': 53,\n",
       " 'friday': 54,\n",
       " 'world': 55,\n",
       " 'than': 56,\n",
       " 'u': 57,\n",
       " '1': 58,\n",
       " 'last': 59,\n",
       " 'they': 60,\n",
       " 'york': 61,\n",
       " 'yesterday': 62,\n",
       " 'against': 63,\n",
       " 'about': 64,\n",
       " 'who': 65,\n",
       " 'not': 66,\n",
       " 'were': 67,\n",
       " 'into': 68,\n",
       " 'out': 69,\n",
       " 'three': 70,\n",
       " 'been': 71,\n",
       " 'president': 72,\n",
       " '2': 73,\n",
       " 'had': 74,\n",
       " 'million': 75,\n",
       " 'corp': 76,\n",
       " 'oil': 77,\n",
       " 'when': 78,\n",
       " 'week': 79,\n",
       " 'time': 80,\n",
       " 'would': 81,\n",
       " 'united': 82,\n",
       " 'sunday': 83,\n",
       " 'which': 84,\n",
       " 'game': 85,\n",
       " 'people': 86,\n",
       " 'today': 87,\n",
       " 'government': 88,\n",
       " 'years': 89,\n",
       " 'could': 90,\n",
       " 'no': 91,\n",
       " 'second': 92,\n",
       " 'group': 93,\n",
       " 'percent': 94,\n",
       " 'n': 95,\n",
       " 'com': 96,\n",
       " 'saturday': 97,\n",
       " 'software': 98,\n",
       " 'next': 99,\n",
       " 'all': 100,\n",
       " 'third': 101,\n",
       " 'season': 102,\n",
       " 'night': 103,\n",
       " 'or': 104,\n",
       " 'prices': 105,\n",
       " 'iraq': 106,\n",
       " 'security': 107,\n",
       " 'day': 108,\n",
       " '3': 109,\n",
       " 'fullquote': 110,\n",
       " 'quarter': 111,\n",
       " 'off': 112,\n",
       " 'stocks': 113,\n",
       " '6': 114,\n",
       " 'microsoft': 115,\n",
       " 'minister': 116,\n",
       " 'announced': 117,\n",
       " 'internet': 118,\n",
       " 'team': 119,\n",
       " 'some': 120,\n",
       " 'four': 121,\n",
       " 'back': 122,\n",
       " 'state': 123,\n",
       " 'international': 124,\n",
       " 'high': 125,\n",
       " 'washington': 126,\n",
       " '2004': 127,\n",
       " 'billion': 128,\n",
       " 'may': 129,\n",
       " 'most': 130,\n",
       " 'market': 131,\n",
       " 'news': 132,\n",
       " 'former': 133,\n",
       " '10': 134,\n",
       " 'officials': 135,\n",
       " 'top': 136,\n",
       " 'can': 137,\n",
       " '4': 138,\n",
       " 'business': 139,\n",
       " 'says': 140,\n",
       " 'other': 141,\n",
       " 'win': 142,\n",
       " 'states': 143,\n",
       " 'if': 144,\n",
       " 'month': 145,\n",
       " '5': 146,\n",
       " 'victory': 147,\n",
       " 'city': 148,\n",
       " 'record': 149,\n",
       " 'end': 150,\n",
       " 'before': 151,\n",
       " 'european': 152,\n",
       " 'open': 153,\n",
       " 'largest': 154,\n",
       " 'technology': 155,\n",
       " 'just': 156,\n",
       " 'co': 157,\n",
       " 'service': 158,\n",
       " 'reported': 159,\n",
       " 't': 160,\n",
       " 'american': 161,\n",
       " 'league': 162,\n",
       " '7': 163,\n",
       " 'sales': 164,\n",
       " 'afp': 165,\n",
       " 'computer': 166,\n",
       " 'home': 167,\n",
       " 'down': 168,\n",
       " 'federal': 169,\n",
       " '000': 170,\n",
       " 'you': 171,\n",
       " 'five': 172,\n",
       " 'killed': 173,\n",
       " 'national': 174,\n",
       " 'according': 175,\n",
       " 'prime': 176,\n",
       " 'what': 177,\n",
       " 'expected': 178,\n",
       " 'made': 179,\n",
       " 'plans': 180,\n",
       " 'research': 181,\n",
       " 'while': 182,\n",
       " '0': 183,\n",
       " 'major': 184,\n",
       " 'ticker': 185,\n",
       " 'network': 186,\n",
       " 'target': 187,\n",
       " 'now': 188,\n",
       " 'during': 189,\n",
       " 'least': 190,\n",
       " 'court': 191,\n",
       " 'online': 192,\n",
       " 'country': 193,\n",
       " 'between': 194,\n",
       " 'chief': 195,\n",
       " 'maker': 196,\n",
       " 'http': 197,\n",
       " 'companies': 198,\n",
       " 'london': 199,\n",
       " 'under': 200,\n",
       " 'them': 201,\n",
       " 'long': 202,\n",
       " 'san': 203,\n",
       " 'www': 204,\n",
       " 'british': 205,\n",
       " 'china': 206,\n",
       " 'set': 207,\n",
       " 'lead': 208,\n",
       " 'cup': 209,\n",
       " 'deal': 210,\n",
       " 'bush': 211,\n",
       " 'final': 212,\n",
       " 'there': 213,\n",
       " 'series': 214,\n",
       " 'href': 215,\n",
       " 'another': 216,\n",
       " 'search': 217,\n",
       " 'make': 218,\n",
       " 'based': 219,\n",
       " 'south': 220,\n",
       " 'bank': 221,\n",
       " 'report': 222,\n",
       " 'since': 223,\n",
       " 'her': 224,\n",
       " 'take': 225,\n",
       " 'police': 226,\n",
       " 'won': 227,\n",
       " 'old': 228,\n",
       " 'investor': 229,\n",
       " 'space': 230,\n",
       " 'giant': 231,\n",
       " 'games': 232,\n",
       " 'coach': 233,\n",
       " 'help': 234,\n",
       " 'john': 235,\n",
       " 'being': 236,\n",
       " 'industry': 237,\n",
       " 'through': 238,\n",
       " 'sports': 239,\n",
       " 'left': 240,\n",
       " 'music': 241,\n",
       " 'shares': 242,\n",
       " 'election': 243,\n",
       " 'leader': 244,\n",
       " 'run': 245,\n",
       " 'him': 246,\n",
       " 'services': 247,\n",
       " 'way': 248,\n",
       " 'web': 249,\n",
       " 'agreed': 250,\n",
       " 'so': 251,\n",
       " 'say': 252,\n",
       " 'only': 253,\n",
       " 'iraqi': 254,\n",
       " 'red': 255,\n",
       " 'because': 256,\n",
       " 'hit': 257,\n",
       " 'early': 258,\n",
       " 'system': 259,\n",
       " 'mobile': 260,\n",
       " 'military': 261,\n",
       " 'six': 262,\n",
       " 'aspx': 263,\n",
       " 'quickinfo': 264,\n",
       " 'o': 265,\n",
       " 'months': 266,\n",
       " 'profit': 267,\n",
       " 'like': 268,\n",
       " 'baghdad': 269,\n",
       " 'general': 270,\n",
       " 'i': 271,\n",
       " 'days': 272,\n",
       " 'north': 273,\n",
       " 'al': 274,\n",
       " 'get': 275,\n",
       " 'rose': 276,\n",
       " 'p': 277,\n",
       " 'war': 278,\n",
       " 'including': 279,\n",
       " 'many': 280,\n",
       " 'union': 281,\n",
       " 'still': 282,\n",
       " 'biggest': 283,\n",
       " 'ago': 284,\n",
       " 'even': 285,\n",
       " 'executive': 286,\n",
       " 'half': 287,\n",
       " 'big': 288,\n",
       " 'talks': 289,\n",
       " 'strong': 290,\n",
       " '8': 291,\n",
       " 'official': 292,\n",
       " 'palestinian': 293,\n",
       " 'trade': 294,\n",
       " 'india': 295,\n",
       " 'plan': 296,\n",
       " 'wireless': 297,\n",
       " 'bid': 298,\n",
       " 'players': 299,\n",
       " 'data': 300,\n",
       " 'round': 301,\n",
       " 'latest': 302,\n",
       " 'phone': 303,\n",
       " 'held': 304,\n",
       " 'higher': 305,\n",
       " 'olympic': 306,\n",
       " 'much': 307,\n",
       " 'start': 308,\n",
       " 'released': 309,\n",
       " 'points': 310,\n",
       " 'stock': 311,\n",
       " 'growth': 312,\n",
       " 'move': 313,\n",
       " 'part': 314,\n",
       " 'nuclear': 315,\n",
       " 'athens': 316,\n",
       " 'earnings': 317,\n",
       " 'google': 318,\n",
       " 'test': 319,\n",
       " 'boston': 320,\n",
       " '11': 321,\n",
       " 'west': 322,\n",
       " '20': 323,\n",
       " 'how': 324,\n",
       " 'where': 325,\n",
       " 'fourth': 326,\n",
       " 'dollar': 327,\n",
       " 'called': 328,\n",
       " 'global': 329,\n",
       " 'head': 330,\n",
       " 'investors': 331,\n",
       " 'play': 332,\n",
       " 'australia': 333,\n",
       " 'public': 334,\n",
       " 'face': 335,\n",
       " 'weeks': 336,\n",
       " 'england': 337,\n",
       " 'israeli': 338,\n",
       " 'users': 339,\n",
       " 'presidential': 340,\n",
       " 'nearly': 341,\n",
       " 'seven': 342,\n",
       " 'economic': 343,\n",
       " 'football': 344,\n",
       " 'found': 345,\n",
       " 'windows': 346,\n",
       " 'japan': 347,\n",
       " 'air': 348,\n",
       " 'nations': 349,\n",
       " 'financial': 350,\n",
       " 'despite': 351,\n",
       " 'ahead': 352,\n",
       " 'francisco': 353,\n",
       " 'cut': 354,\n",
       " 'ibm': 355,\n",
       " 'man': 356,\n",
       " 'free': 357,\n",
       " 'gold': 358,\n",
       " 'took': 359,\n",
       " 'championship': 360,\n",
       " 'around': 361,\n",
       " 'amp': 362,\n",
       " 'foreign': 363,\n",
       " 'fell': 364,\n",
       " 'systems': 365,\n",
       " 'go': 366,\n",
       " '12': 367,\n",
       " 'also': 368,\n",
       " '9': 369,\n",
       " 'street': 370,\n",
       " 'firm': 371,\n",
       " 'russian': 372,\n",
       " 'number': 373,\n",
       " 'should': 374,\n",
       " 'near': 375,\n",
       " 'loss': 376,\n",
       " 'any': 377,\n",
       " 'leading': 378,\n",
       " 'used': 379,\n",
       " 'right': 380,\n",
       " 'forces': 381,\n",
       " 'past': 382,\n",
       " 'reports': 383,\n",
       " 'use': 384,\n",
       " 'following': 385,\n",
       " 'do': 386,\n",
       " 'buy': 387,\n",
       " 'troops': 388,\n",
       " '30': 389,\n",
       " 'quote': 390,\n",
       " 'work': 391,\n",
       " 'key': 392,\n",
       " 'attack': 393,\n",
       " 'sox': 394,\n",
       " 'pay': 395,\n",
       " 'drug': 396,\n",
       " 'economy': 397,\n",
       " 'chicago': 398,\n",
       " 'release': 399,\n",
       " 'agency': 400,\n",
       " 'led': 401,\n",
       " '36': 402,\n",
       " 'share': 403,\n",
       " 'car': 404,\n",
       " 'both': 405,\n",
       " '15': 406,\n",
       " 'per': 407,\n",
       " 'profile': 408,\n",
       " 'version': 409,\n",
       " 'october': 410,\n",
       " 'september': 411,\n",
       " 'press': 412,\n",
       " 'late': 413,\n",
       " 'commission': 414,\n",
       " 'gaza': 415,\n",
       " 'beat': 416,\n",
       " 'video': 417,\n",
       " 'media': 418,\n",
       " 'killing': 419,\n",
       " 'your': 420,\n",
       " '2005': 421,\n",
       " 'price': 422,\n",
       " 'player': 423,\n",
       " 'best': 424,\n",
       " 'rival': 425,\n",
       " 'put': 426,\n",
       " 'several': 427,\n",
       " 'uk': 428,\n",
       " 'apple': 429,\n",
       " 'america': 430,\n",
       " 'well': 431,\n",
       " 'again': 432,\n",
       " 'wall': 433,\n",
       " 'power': 434,\n",
       " 'capital': 435,\n",
       " 'products': 436,\n",
       " 'here': 437,\n",
       " 'tokyo': 438,\n",
       " 'source': 439,\n",
       " 'contract': 440,\n",
       " 'offer': 441,\n",
       " 'close': 442,\n",
       " 'e': 443,\n",
       " 'agreement': 444,\n",
       " 'region': 445,\n",
       " 'such': 446,\n",
       " 'australian': 447,\n",
       " 'recent': 448,\n",
       " 'workers': 449,\n",
       " 'french': 450,\n",
       " 'support': 451,\n",
       " 'might': 452,\n",
       " 'making': 453,\n",
       " 'program': 454,\n",
       " 'conference': 455,\n",
       " 'demand': 456,\n",
       " 'leaders': 457,\n",
       " 'scientists': 458,\n",
       " 'whether': 459,\n",
       " '17': 460,\n",
       " 'title': 461,\n",
       " 'customers': 462,\n",
       " 'crude': 463,\n",
       " 'digital': 464,\n",
       " 'attacks': 465,\n",
       " 'scored': 466,\n",
       " 'un': 467,\n",
       " 'champion': 468,\n",
       " 'jobs': 469,\n",
       " 'b': 470,\n",
       " '14': 471,\n",
       " '18': 472,\n",
       " 'consumer': 473,\n",
       " 'manager': 474,\n",
       " 'lower': 475,\n",
       " 'department': 476,\n",
       " 'party': 477,\n",
       " 'michael': 478,\n",
       " 'nation': 479,\n",
       " 'eight': 480,\n",
       " 'house': 481,\n",
       " 'she': 482,\n",
       " 'saying': 483,\n",
       " 'northern': 484,\n",
       " 'energy': 485,\n",
       " 'japanese': 486,\n",
       " 'life': 487,\n",
       " 'europe': 488,\n",
       " 'center': 489,\n",
       " 'line': 490,\n",
       " 'los': 491,\n",
       " 'november': 492,\n",
       " 'southern': 493,\n",
       " 'running': 494,\n",
       " 'st': 495,\n",
       " 'told': 496,\n",
       " 'low': 497,\n",
       " 'champions': 498,\n",
       " 'little': 499,\n",
       " 'political': 500,\n",
       " 'without': 501,\n",
       " 'meeting': 502,\n",
       " 'management': 503,\n",
       " 'peace': 504,\n",
       " 'field': 505,\n",
       " 'due': 506,\n",
       " 'campaign': 507,\n",
       " 'hurricane': 508,\n",
       " 'decision': 509,\n",
       " 'away': 510,\n",
       " 'place': 511,\n",
       " 'taking': 512,\n",
       " 'launched': 513,\n",
       " 'central': 514,\n",
       " 'angeles': 515,\n",
       " 'russia': 516,\n",
       " 'good': 517,\n",
       " 'canadian': 518,\n",
       " 'winning': 519,\n",
       " 'oracle': 520,\n",
       " 'results': 521,\n",
       " 'florida': 522,\n",
       " 'race': 523,\n",
       " '13': 524,\n",
       " 'university': 525,\n",
       " 'across': 526,\n",
       " 'school': 527,\n",
       " 'office': 528,\n",
       " 'using': 529,\n",
       " 'intel': 530,\n",
       " 'interest': 531,\n",
       " '16': 532,\n",
       " 'star': 533,\n",
       " '25': 534,\n",
       " 'board': 535,\n",
       " 'pakistan': 536,\n",
       " 'give': 537,\n",
       " 'become': 538,\n",
       " 'we': 539,\n",
       " \"world's\": 540,\n",
       " 'net': 541,\n",
       " 'men': 542,\n",
       " 'straight': 543,\n",
       " 'army': 544,\n",
       " 'future': 545,\n",
       " 'morning': 546,\n",
       " 'secretary': 547,\n",
       " 'chairman': 548,\n",
       " 'defense': 549,\n",
       " 'information': 550,\n",
       " 'california': 551,\n",
       " '50': 552,\n",
       " '100': 553,\n",
       " 'match': 554,\n",
       " 'bomb': 555,\n",
       " 'site': 556,\n",
       " 'shot': 557,\n",
       " 'weekend': 558,\n",
       " 'quarterly': 559,\n",
       " 'costs': 560,\n",
       " 'launch': 561,\n",
       " 'lost': 562,\n",
       " 'charges': 563,\n",
       " 'communications': 564,\n",
       " 'linux': 565,\n",
       " 'case': 566,\n",
       " 'elections': 567,\n",
       " 'history': 568,\n",
       " 'houston': 569,\n",
       " 'keep': 570,\n",
       " 'operating': 571,\n",
       " 'oct': 572,\n",
       " 'showed': 573,\n",
       " 'health': 574,\n",
       " 'return': 575,\n",
       " 'better': 576,\n",
       " 'sun': 577,\n",
       " 'own': 578,\n",
       " 'did': 579,\n",
       " 'club': 580,\n",
       " 'same': 581,\n",
       " 'times': 582,\n",
       " 'warned': 583,\n",
       " 'among': 584,\n",
       " 'few': 585,\n",
       " 'posted': 586,\n",
       " 'signed': 587,\n",
       " 'increase': 588,\n",
       " 'death': 589,\n",
       " 'cost': 590,\n",
       " 'senior': 591,\n",
       " 'show': 592,\n",
       " 'france': 593,\n",
       " 'job': 594,\n",
       " 'mark': 595,\n",
       " 'television': 596,\n",
       " 'trading': 597,\n",
       " 'money': 598,\n",
       " 'stores': 599,\n",
       " 'bill': 600,\n",
       " 'those': 601,\n",
       " '24': 602,\n",
       " 'peoplesoft': 603,\n",
       " 'earlier': 604,\n",
       " 'almost': 605,\n",
       " 'grand': 606,\n",
       " 'militants': 607,\n",
       " 'exchange': 608,\n",
       " 'pc': 609,\n",
       " 'ever': 610,\n",
       " 'yankees': 611,\n",
       " 'come': 612,\n",
       " 'force': 613,\n",
       " 'small': 614,\n",
       " 'david': 615,\n",
       " 'east': 616,\n",
       " 'rise': 617,\n",
       " 'stadium': 618,\n",
       " 'font': 619,\n",
       " 'mail': 620,\n",
       " 'less': 621,\n",
       " 'chip': 622,\n",
       " 'accused': 623,\n",
       " '19': 624,\n",
       " 'nasa': 625,\n",
       " 'began': 626,\n",
       " 'nine': 627,\n",
       " 'ltd': 628,\n",
       " 'fans': 629,\n",
       " 'action': 630,\n",
       " 'iran': 631,\n",
       " 'going': 632,\n",
       " 'toronto': 633,\n",
       " 'likely': 634,\n",
       " 'nov': 635,\n",
       " 'texas': 636,\n",
       " 'reserve': 637,\n",
       " 'association': 638,\n",
       " 'baseball': 639,\n",
       " 'came': 640,\n",
       " 'thousands': 641,\n",
       " 'behind': 642,\n",
       " 'ended': 643,\n",
       " 'helped': 644,\n",
       " 'real': 645,\n",
       " 'barrel': 646,\n",
       " 'anti': 647,\n",
       " 'opening': 648,\n",
       " 'offering': 649,\n",
       " 'israel': 650,\n",
       " 'got': 651,\n",
       " 'full': 652,\n",
       " 'battle': 653,\n",
       " 'britain': 654,\n",
       " 'post': 655,\n",
       " 'possible': 656,\n",
       " '21': 657,\n",
       " 'control': 658,\n",
       " 'amid': 659,\n",
       " 'died': 660,\n",
       " 'sell': 661,\n",
       " 'rate': 662,\n",
       " 'trial': 663,\n",
       " 'growing': 664,\n",
       " 'm': 665,\n",
       " 'division': 666,\n",
       " 'too': 667,\n",
       " 'august': 668,\n",
       " 'cash': 669,\n",
       " 'judge': 670,\n",
       " 'already': 671,\n",
       " 'point': 672,\n",
       " 'korea': 673,\n",
       " 'darfur': 674,\n",
       " 'access': 675,\n",
       " 'yet': 676,\n",
       " 'paris': 677,\n",
       " 'medal': 678,\n",
       " 'term': 679,\n",
       " 'food': 680,\n",
       " 'tax': 681,\n",
       " 'performance': 682,\n",
       " 'members': 683,\n",
       " 'hopes': 684,\n",
       " 'hours': 685,\n",
       " 'kerry': 686,\n",
       " 'bankruptcy': 687,\n",
       " 'countries': 688,\n",
       " 'far': 689,\n",
       " 'white': 690,\n",
       " 'once': 691,\n",
       " 'women': 692,\n",
       " 'administration': 693,\n",
       " 'sept': 694,\n",
       " 'revenue': 695,\n",
       " 'career': 696,\n",
       " 'fall': 697,\n",
       " '151': 698,\n",
       " 'injured': 699,\n",
       " 'rates': 700,\n",
       " 'networks': 701,\n",
       " 'trying': 702,\n",
       " 'western': 703,\n",
       " 'production': 704,\n",
       " 'gave': 705,\n",
       " 'store': 706,\n",
       " 'study': 707,\n",
       " 'efforts': 708,\n",
       " 'airlines': 709,\n",
       " 'chinese': 710,\n",
       " 'within': 711,\n",
       " 'see': 712,\n",
       " 'authorities': 713,\n",
       " 'soon': 714,\n",
       " 'violence': 715,\n",
       " 'yards': 716,\n",
       " 'williams': 717,\n",
       " 'product': 718,\n",
       " 'computers': 719,\n",
       " 'german': 720,\n",
       " 'corporate': 721,\n",
       " 'hard': 722,\n",
       " 'look': 723,\n",
       " 'germany': 724,\n",
       " 'soldiers': 725,\n",
       " 'local': 726,\n",
       " 'george': 727,\n",
       " 'speed': 728,\n",
       " 'human': 729,\n",
       " 'radio': 730,\n",
       " 'server': 731,\n",
       " 'hostage': 732,\n",
       " 'takeover': 733,\n",
       " 'step': 734,\n",
       " 'canada': 735,\n",
       " 'change': 736,\n",
       " 'main': 737,\n",
       " 'starting': 738,\n",
       " 'popular': 739,\n",
       " 'opposition': 740,\n",
       " 'personal': 741,\n",
       " 'outside': 742,\n",
       " 'competition': 743,\n",
       " 'paul': 744,\n",
       " 'aimed': 745,\n",
       " 'each': 746,\n",
       " 'quarterback': 747,\n",
       " 'markets': 748,\n",
       " 'fund': 749,\n",
       " 'reached': 750,\n",
       " 'investment': 751,\n",
       " 'african': 752,\n",
       " 'fire': 753,\n",
       " 'size': 754,\n",
       " 'law': 755,\n",
       " 'until': 756,\n",
       " 'arafat': 757,\n",
       " 'short': 758,\n",
       " 'concerns': 759,\n",
       " 'strike': 760,\n",
       " 'engine': 761,\n",
       " 'known': 762,\n",
       " 'continued': 763,\n",
       " 'phones': 764,\n",
       " 'failed': 765,\n",
       " 'jones': 766,\n",
       " 'vote': 767,\n",
       " 'enough': 768,\n",
       " 'devices': 769,\n",
       " 'giants': 770,\n",
       " 'yasser': 771,\n",
       " 'retailer': 772,\n",
       " 'gas': 773,\n",
       " 'afghanistan': 774,\n",
       " 'then': 775,\n",
       " 'analysts': 776,\n",
       " 'taken': 777,\n",
       " 'begin': 778,\n",
       " 'others': 779,\n",
       " 'development': 780,\n",
       " 'tour': 781,\n",
       " 'profits': 782,\n",
       " 'looking': 783,\n",
       " 'dead': 784,\n",
       " 'manchester': 785,\n",
       " 'filed': 786,\n",
       " 'weapons': 787,\n",
       " 'tech': 788,\n",
       " 'fight': 789,\n",
       " 'airline': 790,\n",
       " 'insurance': 791,\n",
       " 'selling': 792,\n",
       " 'stop': 793,\n",
       " 'reach': 794,\n",
       " 'council': 795,\n",
       " 'designed': 796,\n",
       " 'unit': 797,\n",
       " 'miami': 798,\n",
       " 'raised': 799,\n",
       " 'seattle': 800,\n",
       " 'desktop': 801,\n",
       " 'large': 802,\n",
       " 'teams': 803,\n",
       " 'sony': 804,\n",
       " 'cell': 805,\n",
       " 'americans': 806,\n",
       " 'bay': 807,\n",
       " 'beijing': 808,\n",
       " 'side': 809,\n",
       " 're': 810,\n",
       " 'electronics': 811,\n",
       " 'executives': 812,\n",
       " 'meet': 813,\n",
       " 'indian': 814,\n",
       " 'great': 815,\n",
       " 'olympics': 816,\n",
       " 'find': 817,\n",
       " '500': 818,\n",
       " \"it's\": 819,\n",
       " 'went': 820,\n",
       " 'legal': 821,\n",
       " 'must': 822,\n",
       " 'effort': 823,\n",
       " 'hold': 824,\n",
       " 'rebel': 825,\n",
       " 'operations': 826,\n",
       " 'fuel': 827,\n",
       " 'further': 828,\n",
       " 'finally': 829,\n",
       " 'cp': 830,\n",
       " 'green': 831,\n",
       " 'fighting': 832,\n",
       " 'tony': 833,\n",
       " 'powerful': 834,\n",
       " 'coming': 835,\n",
       " 'consumers': 836,\n",
       " 'researchers': 837,\n",
       " 'working': 838,\n",
       " 'level': 839,\n",
       " 'spending': 840,\n",
       " 'file': 841,\n",
       " 'coast': 842,\n",
       " 'call': 843,\n",
       " 'moscow': 844,\n",
       " 'need': 845,\n",
       " 'rights': 846,\n",
       " 'area': 847,\n",
       " 'toward': 848,\n",
       " 'fired': 849,\n",
       " 'madrid': 850,\n",
       " 'claims': 851,\n",
       " 'atlanta': 852,\n",
       " 'family': 853,\n",
       " 'annual': 854,\n",
       " 'forecast': 855,\n",
       " 'getting': 856,\n",
       " 'securities': 857,\n",
       " 'continue': 858,\n",
       " 'businesses': 859,\n",
       " '23': 860,\n",
       " 'allow': 861,\n",
       " 'rally': 862,\n",
       " 'private': 863,\n",
       " 'never': 864,\n",
       " 'euro': 865,\n",
       " 'arrested': 866,\n",
       " 'station': 867,\n",
       " 'goal': 868,\n",
       " 'tv': 869,\n",
       " 'list': 870,\n",
       " 'bowl': 871,\n",
       " 'blue': 872,\n",
       " \"'s\": 873,\n",
       " 'heart': 874,\n",
       " 'yahoo': 875,\n",
       " 'every': 876,\n",
       " 'labor': 877,\n",
       " 'provide': 878,\n",
       " 'cbs': 879,\n",
       " 'pressure': 880,\n",
       " 'rebels': 881,\n",
       " 'democratic': 882,\n",
       " 'africa': 883,\n",
       " '27': 884,\n",
       " 'visit': 885,\n",
       " 'planned': 886,\n",
       " 'children': 887,\n",
       " 'calif': 888,\n",
       " 'italian': 889,\n",
       " 'holiday': 890,\n",
       " 'investigation': 891,\n",
       " 'basketball': 892,\n",
       " 'drop': 893,\n",
       " 'drive': 894,\n",
       " 'name': 895,\n",
       " 'sent': 896,\n",
       " 'spain': 897,\n",
       " 'available': 898,\n",
       " 'boost': 899,\n",
       " 'hundreds': 900,\n",
       " 'survey': 901,\n",
       " '28': 902,\n",
       " 'current': 903,\n",
       " 'charged': 904,\n",
       " 'regulators': 905,\n",
       " 'attempt': 906,\n",
       " 'named': 907,\n",
       " 'dollars': 908,\n",
       " 'officer': 909,\n",
       " 'expectations': 910,\n",
       " 'policy': 911,\n",
       " 'muslim': 912,\n",
       " 'rising': 913,\n",
       " '22': 914,\n",
       " 'college': 915,\n",
       " 'problems': 916,\n",
       " 'sudan': 917,\n",
       " 'asked': 918,\n",
       " 'suspected': 919,\n",
       " 'approved': 920,\n",
       " 'ipod': 921,\n",
       " 'sold': 922,\n",
       " 'nasdaq': 923,\n",
       " 'detroit': 924,\n",
       " 'december': 925,\n",
       " 'gains': 926,\n",
       " 'decided': 927,\n",
       " 'aid': 928,\n",
       " 'supply': 929,\n",
       " 'ceo': 930,\n",
       " 'park': 931,\n",
       " 'employees': 932,\n",
       " 'philadelphia': 933,\n",
       " 'retail': 934,\n",
       " 'nfl': 935,\n",
       " 'flight': 936,\n",
       " 'unveiled': 937,\n",
       " 'worth': 938,\n",
       " 'islamic': 939,\n",
       " 'confirmed': 940,\n",
       " 'tournament': 941,\n",
       " 'arsenal': 942,\n",
       " 'turned': 943,\n",
       " 'seen': 944,\n",
       " 'rivals': 945,\n",
       " 'sale': 946,\n",
       " 'eu': 947,\n",
       " 'equipment': 948,\n",
       " 'louis': 949,\n",
       " 'injury': 950,\n",
       " 'strip': 951,\n",
       " 'order': 952,\n",
       " 'claimed': 953,\n",
       " 'sign': 954,\n",
       " 'holding': 955,\n",
       " 'ruling': 956,\n",
       " 'later': 957,\n",
       " 'technologies': 958,\n",
       " 'town': 959,\n",
       " 'winter': 960,\n",
       " 'increased': 961,\n",
       " 'try': 962,\n",
       " 'terrorism': 963,\n",
       " 'giving': 964,\n",
       " 'color': 965,\n",
       " 'staff': 966,\n",
       " 'moved': 967,\n",
       " 'spokesman': 968,\n",
       " 'stake': 969,\n",
       " 'want': 970,\n",
       " 'guard': 971,\n",
       " 'fresh': 972,\n",
       " 'though': 973,\n",
       " '40': 974,\n",
       " 'opened': 975,\n",
       " 'mike': 976,\n",
       " 'fifth': 977,\n",
       " 'generation': 978,\n",
       " 'above': 979,\n",
       " 'cent': 980,\n",
       " 'average': 981,\n",
       " 'w': 982,\n",
       " 'received': 983,\n",
       " 'threat': 984,\n",
       " 'inning': 985,\n",
       " 'chance': 986,\n",
       " 'double': 987,\n",
       " 'hospital': 988,\n",
       " 'touchdown': 989,\n",
       " 'added': 990,\n",
       " 'ranked': 991,\n",
       " 'steve': 992,\n",
       " '2006': 993,\n",
       " 'crisis': 994,\n",
       " 'process': 995,\n",
       " 'applications': 996,\n",
       " 'captain': 997,\n",
       " '2003': 998,\n",
       " 'suicide': 999,\n",
       " 'black': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(news)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(news)\n",
    "N = len(sequences)\n",
    "List = list()\n",
    "for i in range(N):\n",
    "    List.append([sequences[i], y_train[i]])\n",
    "#sequences2 = sorted(sequences, key=lambda seq: len(seq))\n",
    "List2 = sorted(List, key=lambda data: len(data[0]))\n",
    "sequences2 = list()\n",
    "y_train2 = np.ndarray(shape = y_train.shape, dtype = np.float32())\n",
    "for i in range(N):\n",
    "    sequences2.append(List2[i][0])\n",
    "    y_train2[i] = np.float32(List2[i][1])\n",
    "    #print(List2[i][1],y_train2[i],i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(sequences, maxlen=max_news_len, padding = 'post')\n",
    "x_train2 = pad_sequences(sequences2, maxlen=max_news_len, padding = 'post')\n",
    "#x_train = pad_sequences(sequences, maxlen=max_news_len)\n",
    "#x_train2 = pad_sequences(sequences2, maxlen=max_news_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.9 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 30, 32)            320000    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 16)                3136      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 323,204\n",
      "Trainable params: 323,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "l2_lambda = 0.0001\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(num_words, 32, input_length=max_news_len))\n",
    "model_lstm.add(Dropout(0.9))\n",
    "model_lstm.add(LSTM(16, dropout=0.5, recurrent_dropout=0.2, kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "#model_lstm.add(Dropout(0.9))\n",
    "model_lstm.add(Dense(4, activation='softmax'))\n",
    "model_lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_lstm.summary()\n",
    "#print(x_train[1])\n",
    "#print(y_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 108000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Large dropout rate: 0.9 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "108000/108000 [==============================] - 19s 173us/sample - loss: 1.3594 - accuracy: 0.3302 - val_loss: 1.1126 - val_accuracy: 0.5033\n",
      "Epoch 2/50\n",
      "108000/108000 [==============================] - 10s 93us/sample - loss: 1.0726 - accuracy: 0.5172 - val_loss: 0.8210 - val_accuracy: 0.6224\n",
      "Epoch 3/50\n",
      "108000/108000 [==============================] - 10s 92us/sample - loss: 0.8743 - accuracy: 0.6145 - val_loss: 0.7098 - val_accuracy: 0.6823\n",
      "Epoch 4/50\n",
      "108000/108000 [==============================] - 10s 94us/sample - loss: 0.7700 - accuracy: 0.6635 - val_loss: 0.6694 - val_accuracy: 0.7583\n",
      "Epoch 5/50\n",
      "108000/108000 [==============================] - 10s 94us/sample - loss: 0.7155 - accuracy: 0.6949 - val_loss: 0.6268 - val_accuracy: 0.7962\n",
      "Epoch 6/50\n",
      "108000/108000 [==============================] - 10s 94us/sample - loss: 0.6697 - accuracy: 0.7288 - val_loss: 0.5691 - val_accuracy: 0.8317\n",
      "Epoch 7/50\n",
      "108000/108000 [==============================] - 10s 93us/sample - loss: 0.6271 - accuracy: 0.7600 - val_loss: 0.5198 - val_accuracy: 0.8446\n",
      "Epoch 8/50\n",
      "108000/108000 [==============================] - 10s 95us/sample - loss: 0.5920 - accuracy: 0.7811 - val_loss: 0.4997 - val_accuracy: 0.8474\n",
      "Epoch 9/50\n",
      "108000/108000 [==============================] - 10s 92us/sample - loss: 0.5646 - accuracy: 0.7950 - val_loss: 0.4823 - val_accuracy: 0.8532\n",
      "Epoch 10/50\n",
      "108000/108000 [==============================] - 10s 93us/sample - loss: 0.5480 - accuracy: 0.8029 - val_loss: 0.4745 - val_accuracy: 0.8544\n",
      "Epoch 11/50\n",
      "108000/108000 [==============================] - 10s 92us/sample - loss: 0.5280 - accuracy: 0.8107 - val_loss: 0.4689 - val_accuracy: 0.8571\n",
      "Epoch 12/50\n",
      "108000/108000 [==============================] - 10s 93us/sample - loss: 0.5177 - accuracy: 0.8160 - val_loss: 0.4660 - val_accuracy: 0.8555\n",
      "Epoch 13/50\n",
      "108000/108000 [==============================] - 10s 93us/sample - loss: 0.5087 - accuracy: 0.8201 - val_loss: 0.4593 - val_accuracy: 0.8581\n",
      "Epoch 14/50\n",
      "108000/108000 [==============================] - 10s 92us/sample - loss: 0.4999 - accuracy: 0.8240 - val_loss: 0.4555 - val_accuracy: 0.8594\n",
      "Epoch 15/50\n",
      "108000/108000 [==============================] - 10s 92us/sample - loss: 0.4939 - accuracy: 0.8256 - val_loss: 0.4531 - val_accuracy: 0.8615\n",
      "Epoch 16/50\n",
      "108000/108000 [==============================] - 10s 93us/sample - loss: 0.4830 - accuracy: 0.8301 - val_loss: 0.4532 - val_accuracy: 0.8603\n",
      "Epoch 17/50\n",
      "108000/108000 [==============================] - 10s 96us/sample - loss: 0.4771 - accuracy: 0.8321 - val_loss: 0.4535 - val_accuracy: 0.8602\n",
      "Epoch 18/50\n",
      "108000/108000 [==============================] - 10s 95us/sample - loss: 0.4722 - accuracy: 0.8340 - val_loss: 0.4474 - val_accuracy: 0.8613\n",
      "Epoch 19/50\n",
      "108000/108000 [==============================] - 10s 94us/sample - loss: 0.4689 - accuracy: 0.8352 - val_loss: 0.4470 - val_accuracy: 0.8626\n",
      "Epoch 20/50\n",
      "108000/108000 [==============================] - 10s 95us/sample - loss: 0.4629 - accuracy: 0.8380 - val_loss: 0.4466 - val_accuracy: 0.8622\n",
      "Epoch 21/50\n",
      "108000/108000 [==============================] - 10s 93us/sample - loss: 0.4575 - accuracy: 0.8392 - val_loss: 0.4431 - val_accuracy: 0.8648\n",
      "Epoch 22/50\n",
      "108000/108000 [==============================] - 10s 93us/sample - loss: 0.4522 - accuracy: 0.8412 - val_loss: 0.4438 - val_accuracy: 0.8638\n",
      "Epoch 23/50\n",
      "108000/108000 [==============================] - 10s 97us/sample - loss: 0.4457 - accuracy: 0.8431 - val_loss: 0.4456 - val_accuracy: 0.8639\n",
      "Epoch 24/50\n",
      "108000/108000 [==============================] - 10s 93us/sample - loss: 0.4469 - accuracy: 0.8438 - val_loss: 0.4457 - val_accuracy: 0.8630\n",
      "Epoch 25/50\n",
      "108000/108000 [==============================] - 10s 93us/sample - loss: 0.4439 - accuracy: 0.8445 - val_loss: 0.4438 - val_accuracy: 0.8640\n",
      "Epoch 26/50\n",
      "108000/108000 [==============================] - 10s 93us/sample - loss: 0.4446 - accuracy: 0.8440 - val_loss: 0.4386 - val_accuracy: 0.8658\n",
      "Epoch 27/50\n",
      "108000/108000 [==============================] - 10s 93us/sample - loss: 0.4384 - accuracy: 0.8477 - val_loss: 0.4393 - val_accuracy: 0.8643\n",
      "Epoch 28/50\n",
      "108000/108000 [==============================] - 10s 94us/sample - loss: 0.4345 - accuracy: 0.8494 - val_loss: 0.4403 - val_accuracy: 0.8642\n",
      "Epoch 29/50\n",
      "108000/108000 [==============================] - 10s 95us/sample - loss: 0.4320 - accuracy: 0.8489 - val_loss: 0.4396 - val_accuracy: 0.8653\n",
      "Epoch 30/50\n",
      "108000/108000 [==============================] - 10s 96us/sample - loss: 0.4306 - accuracy: 0.8493 - val_loss: 0.4386 - val_accuracy: 0.8652\n",
      "Epoch 31/50\n",
      "108000/108000 [==============================] - 10s 95us/sample - loss: 0.4292 - accuracy: 0.8507 - val_loss: 0.4406 - val_accuracy: 0.8645\n"
     ]
    }
   ],
   "source": [
    "history_lstm = model_lstm.fit(x_train2, y_train2, epochs=50, batch_size=1536, validation_split=0.1, callbacks=[EarlyStopping(monitor='val_loss', patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_lstm.history['accuracy'], \n",
    "         label='Доля верных ответов на обучающем наборе')\n",
    "plt.plot(history_lstm.history['val_accuracy'], \n",
    "         label='Доля верных ответов на проверочном наборе')\n",
    "plt.xlabel('Эпоха обучения')\n",
    "plt.ylabel('Доля верных ответов')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.32516818248911905 \n",
      "val_accuracy: 0.8975\n"
     ]
    }
   ],
   "source": [
    "model_lstm.save('LSTM.h5')\n",
    "test = pd.read_csv('test.csv', header=None, names=['class', 'title', 'text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test['text'])\n",
    "x_test = pad_sequences(test_sequences, maxlen=max_news_len, padding = 'post')\n",
    "y_test = utils.to_categorical(test['class'] - 1, nb_classes)\n",
    "res = model_lstm.evaluate(x_test, y_test, verbose=0)\n",
    "print('val_loss:',res[0],'\\nval_accuracy:',res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160354</td>\n",
       "      <td>249</td>\n",
       "      <td>251</td>\n",
       "      <td>9.9998</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>100.0</td>\n",
       "      <td>489.05</td>\n",
       "      <td>605.33</td>\n",
       "      <td>1516.36</td>\n",
       "      <td>1315.28</td>\n",
       "      <td>10.52</td>\n",
       "      <td>...</td>\n",
       "      <td>8185.69</td>\n",
       "      <td>8.4541</td>\n",
       "      <td>0.03</td>\n",
       "      <td>372</td>\n",
       "      <td>2319</td>\n",
       "      <td>100.0</td>\n",
       "      <td>29.11</td>\n",
       "      <td>17.5234</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160355</td>\n",
       "      <td>249</td>\n",
       "      <td>252</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.42</td>\n",
       "      <td>1598.92</td>\n",
       "      <td>1426.77</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8185.47</td>\n",
       "      <td>8.2221</td>\n",
       "      <td>0.03</td>\n",
       "      <td>396</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.38</td>\n",
       "      <td>23.7151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160356</td>\n",
       "      <td>249</td>\n",
       "      <td>253</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.68</td>\n",
       "      <td>1607.72</td>\n",
       "      <td>1430.56</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>8193.94</td>\n",
       "      <td>8.2525</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.78</td>\n",
       "      <td>23.8270</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160357</td>\n",
       "      <td>249</td>\n",
       "      <td>254</td>\n",
       "      <td>35.0046</td>\n",
       "      <td>0.8400</td>\n",
       "      <td>100.0</td>\n",
       "      <td>449.44</td>\n",
       "      <td>555.77</td>\n",
       "      <td>1381.29</td>\n",
       "      <td>1148.18</td>\n",
       "      <td>5.48</td>\n",
       "      <td>...</td>\n",
       "      <td>8125.64</td>\n",
       "      <td>9.0515</td>\n",
       "      <td>0.02</td>\n",
       "      <td>337</td>\n",
       "      <td>2223</td>\n",
       "      <td>100.0</td>\n",
       "      <td>15.26</td>\n",
       "      <td>9.0774</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160358</td>\n",
       "      <td>249</td>\n",
       "      <td>255</td>\n",
       "      <td>42.0030</td>\n",
       "      <td>0.8400</td>\n",
       "      <td>100.0</td>\n",
       "      <td>445.00</td>\n",
       "      <td>549.85</td>\n",
       "      <td>1369.75</td>\n",
       "      <td>1147.45</td>\n",
       "      <td>3.91</td>\n",
       "      <td>...</td>\n",
       "      <td>8144.33</td>\n",
       "      <td>9.1207</td>\n",
       "      <td>0.02</td>\n",
       "      <td>333</td>\n",
       "      <td>2212</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10.66</td>\n",
       "      <td>6.4341</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160359 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0    1        2       3      4       5       6        7        8   \\\n",
       "0         1    1  -0.0007 -0.0004  100.0  518.67  641.82  1589.70  1400.60   \n",
       "1         1    2   0.0019 -0.0003  100.0  518.67  642.15  1591.82  1403.14   \n",
       "2         1    3  -0.0043  0.0003  100.0  518.67  642.35  1587.99  1404.20   \n",
       "3         1    4   0.0007  0.0000  100.0  518.67  642.35  1582.79  1401.87   \n",
       "4         1    5  -0.0019 -0.0002  100.0  518.67  642.37  1582.85  1406.22   \n",
       "...     ...  ...      ...     ...    ...     ...     ...      ...      ...   \n",
       "160354  249  251   9.9998  0.2500  100.0  489.05  605.33  1516.36  1315.28   \n",
       "160355  249  252   0.0028  0.0015  100.0  518.67  643.42  1598.92  1426.77   \n",
       "160356  249  253   0.0029  0.0000  100.0  518.67  643.68  1607.72  1430.56   \n",
       "160357  249  254  35.0046  0.8400  100.0  449.44  555.77  1381.29  1148.18   \n",
       "160358  249  255  42.0030  0.8400  100.0  445.00  549.85  1369.75  1147.45   \n",
       "\n",
       "           9   ...       18      19    20   21    22     23     24       25  \\\n",
       "0       14.62  ...  8138.62  8.4195  0.03  392  2388  100.0  39.06  23.4190   \n",
       "1       14.62  ...  8131.49  8.4318  0.03  392  2388  100.0  39.00  23.4236   \n",
       "2       14.62  ...  8133.23  8.4178  0.03  390  2388  100.0  38.95  23.3442   \n",
       "3       14.62  ...  8133.83  8.3682  0.03  392  2388  100.0  38.88  23.3739   \n",
       "4       14.62  ...  8133.80  8.4294  0.03  393  2388  100.0  38.90  23.4044   \n",
       "...       ...  ...      ...     ...   ...  ...   ...    ...    ...      ...   \n",
       "160354  10.52  ...  8185.69  8.4541  0.03  372  2319  100.0  29.11  17.5234   \n",
       "160355  14.62  ...  8185.47  8.2221  0.03  396  2388  100.0  39.38  23.7151   \n",
       "160356  14.62  ...  8193.94  8.2525  0.03  395  2388  100.0  39.78  23.8270   \n",
       "160357   5.48  ...  8125.64  9.0515  0.02  337  2223  100.0  15.26   9.0774   \n",
       "160358   3.91  ...  8144.33  9.1207  0.02  333  2212  100.0  10.66   6.4341   \n",
       "\n",
       "        26  27  \n",
       "0      NaN NaN  \n",
       "1      NaN NaN  \n",
       "2      NaN NaN  \n",
       "3      NaN NaN  \n",
       "4      NaN NaN  \n",
       "...     ..  ..  \n",
       "160354 NaN NaN  \n",
       "160355 NaN NaN  \n",
       "160356 NaN NaN  \n",
       "160357 NaN NaN  \n",
       "160358 NaN NaN  \n",
       "\n",
       "[160359 rows x 28 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Задание 2\n",
    "names = str(list(range(26)))\n",
    "print(names)\n",
    "train_data = pd.read_csv('./CMAPSSData/train_FD.txt', sep = ' ', header=None)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>554.36</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>9046.19</td>\n",
       "      <td>47.47</td>\n",
       "      <td>521.66</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>392</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>553.75</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>9044.07</td>\n",
       "      <td>47.49</td>\n",
       "      <td>522.28</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>392</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>554.26</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>9052.94</td>\n",
       "      <td>47.27</td>\n",
       "      <td>522.42</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>390</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>554.45</td>\n",
       "      <td>2388.11</td>\n",
       "      <td>9049.48</td>\n",
       "      <td>47.13</td>\n",
       "      <td>522.86</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>392</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>554.00</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>9055.15</td>\n",
       "      <td>47.28</td>\n",
       "      <td>522.19</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>393</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160354</td>\n",
       "      <td>249</td>\n",
       "      <td>251</td>\n",
       "      <td>9.9998</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>605.33</td>\n",
       "      <td>1516.36</td>\n",
       "      <td>1315.28</td>\n",
       "      <td>404.59</td>\n",
       "      <td>2319.66</td>\n",
       "      <td>8840.16</td>\n",
       "      <td>46.08</td>\n",
       "      <td>380.16</td>\n",
       "      <td>2388.73</td>\n",
       "      <td>8185.69</td>\n",
       "      <td>8.4541</td>\n",
       "      <td>372</td>\n",
       "      <td>29.11</td>\n",
       "      <td>17.5234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160355</td>\n",
       "      <td>249</td>\n",
       "      <td>252</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>643.42</td>\n",
       "      <td>1598.92</td>\n",
       "      <td>1426.77</td>\n",
       "      <td>567.59</td>\n",
       "      <td>2388.47</td>\n",
       "      <td>9117.12</td>\n",
       "      <td>48.04</td>\n",
       "      <td>535.02</td>\n",
       "      <td>2388.46</td>\n",
       "      <td>8185.47</td>\n",
       "      <td>8.2221</td>\n",
       "      <td>396</td>\n",
       "      <td>39.38</td>\n",
       "      <td>23.7151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160356</td>\n",
       "      <td>249</td>\n",
       "      <td>253</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>643.68</td>\n",
       "      <td>1607.72</td>\n",
       "      <td>1430.56</td>\n",
       "      <td>569.04</td>\n",
       "      <td>2388.51</td>\n",
       "      <td>9126.53</td>\n",
       "      <td>48.24</td>\n",
       "      <td>535.41</td>\n",
       "      <td>2388.48</td>\n",
       "      <td>8193.94</td>\n",
       "      <td>8.2525</td>\n",
       "      <td>395</td>\n",
       "      <td>39.78</td>\n",
       "      <td>23.8270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160357</td>\n",
       "      <td>249</td>\n",
       "      <td>254</td>\n",
       "      <td>35.0046</td>\n",
       "      <td>0.8400</td>\n",
       "      <td>555.77</td>\n",
       "      <td>1381.29</td>\n",
       "      <td>1148.18</td>\n",
       "      <td>199.93</td>\n",
       "      <td>2223.78</td>\n",
       "      <td>8403.64</td>\n",
       "      <td>42.53</td>\n",
       "      <td>187.92</td>\n",
       "      <td>2388.83</td>\n",
       "      <td>8125.64</td>\n",
       "      <td>9.0515</td>\n",
       "      <td>337</td>\n",
       "      <td>15.26</td>\n",
       "      <td>9.0774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160358</td>\n",
       "      <td>249</td>\n",
       "      <td>255</td>\n",
       "      <td>42.0030</td>\n",
       "      <td>0.8400</td>\n",
       "      <td>549.85</td>\n",
       "      <td>1369.75</td>\n",
       "      <td>1147.45</td>\n",
       "      <td>142.47</td>\n",
       "      <td>2212.52</td>\n",
       "      <td>8391.31</td>\n",
       "      <td>42.60</td>\n",
       "      <td>134.32</td>\n",
       "      <td>2388.66</td>\n",
       "      <td>8144.33</td>\n",
       "      <td>9.1207</td>\n",
       "      <td>333</td>\n",
       "      <td>10.66</td>\n",
       "      <td>6.4341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160359 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1        2       3       4        5        6       7        8  \\\n",
       "0         1    1  -0.0007 -0.0004  641.82  1589.70  1400.60  554.36  2388.06   \n",
       "1         1    2   0.0019 -0.0003  642.15  1591.82  1403.14  553.75  2388.04   \n",
       "2         1    3  -0.0043  0.0003  642.35  1587.99  1404.20  554.26  2388.08   \n",
       "3         1    4   0.0007  0.0000  642.35  1582.79  1401.87  554.45  2388.11   \n",
       "4         1    5  -0.0019 -0.0002  642.37  1582.85  1406.22  554.00  2388.06   \n",
       "...     ...  ...      ...     ...     ...      ...      ...     ...      ...   \n",
       "160354  249  251   9.9998  0.2500  605.33  1516.36  1315.28  404.59  2319.66   \n",
       "160355  249  252   0.0028  0.0015  643.42  1598.92  1426.77  567.59  2388.47   \n",
       "160356  249  253   0.0029  0.0000  643.68  1607.72  1430.56  569.04  2388.51   \n",
       "160357  249  254  35.0046  0.8400  555.77  1381.29  1148.18  199.93  2223.78   \n",
       "160358  249  255  42.0030  0.8400  549.85  1369.75  1147.45  142.47  2212.52   \n",
       "\n",
       "              9     10      11       12       13      14   15     16       17  \n",
       "0       9046.19  47.47  521.66  2388.02  8138.62  8.4195  392  39.06  23.4190  \n",
       "1       9044.07  47.49  522.28  2388.07  8131.49  8.4318  392  39.00  23.4236  \n",
       "2       9052.94  47.27  522.42  2388.03  8133.23  8.4178  390  38.95  23.3442  \n",
       "3       9049.48  47.13  522.86  2388.08  8133.83  8.3682  392  38.88  23.3739  \n",
       "4       9055.15  47.28  522.19  2388.04  8133.80  8.4294  393  38.90  23.4044  \n",
       "...         ...    ...     ...      ...      ...     ...  ...    ...      ...  \n",
       "160354  8840.16  46.08  380.16  2388.73  8185.69  8.4541  372  29.11  17.5234  \n",
       "160355  9117.12  48.04  535.02  2388.46  8185.47  8.2221  396  39.38  23.7151  \n",
       "160356  9126.53  48.24  535.41  2388.48  8193.94  8.2525  395  39.78  23.8270  \n",
       "160357  8403.64  42.53  187.92  2388.83  8125.64  9.0515  337  15.26   9.0774  \n",
       "160358  8391.31  42.60  134.32  2388.66  8144.33  9.1207  333  10.66   6.4341  \n",
       "\n",
       "[160359 rows x 18 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mas = [4, 5, 9, 10, 14, 20, 22, 23, 26, 27]\n",
    "train_data2 = train_data.drop(train_data.columns[mas], axis=1)\n",
    "train_data2.columns = range(18)\n",
    "train_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.701701</td>\n",
       "      <td>-1.041427</td>\n",
       "      <td>-1.111321</td>\n",
       "      <td>1.046626</td>\n",
       "      <td>1.037990</td>\n",
       "      <td>1.024534</td>\n",
       "      <td>1.117707</td>\n",
       "      <td>0.802032</td>\n",
       "      <td>0.983932</td>\n",
       "      <td>0.950816</td>\n",
       "      <td>1.113752</td>\n",
       "      <td>0.345199</td>\n",
       "      <td>0.616065</td>\n",
       "      <td>-0.844470</td>\n",
       "      <td>1.009021</td>\n",
       "      <td>1.121958</td>\n",
       "      <td>1.119482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.701701</td>\n",
       "      <td>-1.041270</td>\n",
       "      <td>-1.111050</td>\n",
       "      <td>1.054394</td>\n",
       "      <td>1.055929</td>\n",
       "      <td>1.043169</td>\n",
       "      <td>1.114204</td>\n",
       "      <td>0.801891</td>\n",
       "      <td>0.978273</td>\n",
       "      <td>0.956653</td>\n",
       "      <td>1.117528</td>\n",
       "      <td>0.345649</td>\n",
       "      <td>0.527629</td>\n",
       "      <td>-0.828118</td>\n",
       "      <td>1.009021</td>\n",
       "      <td>1.116826</td>\n",
       "      <td>1.120138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.701701</td>\n",
       "      <td>-1.041645</td>\n",
       "      <td>-1.109426</td>\n",
       "      <td>1.059103</td>\n",
       "      <td>1.023520</td>\n",
       "      <td>1.050946</td>\n",
       "      <td>1.117133</td>\n",
       "      <td>0.802172</td>\n",
       "      <td>1.001948</td>\n",
       "      <td>0.892447</td>\n",
       "      <td>1.118380</td>\n",
       "      <td>0.345289</td>\n",
       "      <td>0.549211</td>\n",
       "      <td>-0.846729</td>\n",
       "      <td>0.944549</td>\n",
       "      <td>1.112549</td>\n",
       "      <td>1.108820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.701701</td>\n",
       "      <td>-1.041342</td>\n",
       "      <td>-1.110238</td>\n",
       "      <td>1.059103</td>\n",
       "      <td>0.979517</td>\n",
       "      <td>1.033851</td>\n",
       "      <td>1.118224</td>\n",
       "      <td>0.802383</td>\n",
       "      <td>0.992713</td>\n",
       "      <td>0.851589</td>\n",
       "      <td>1.121060</td>\n",
       "      <td>0.345739</td>\n",
       "      <td>0.556653</td>\n",
       "      <td>-0.912666</td>\n",
       "      <td>1.009021</td>\n",
       "      <td>1.106562</td>\n",
       "      <td>1.113053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.701701</td>\n",
       "      <td>-1.041500</td>\n",
       "      <td>-1.110779</td>\n",
       "      <td>1.059573</td>\n",
       "      <td>0.980025</td>\n",
       "      <td>1.065766</td>\n",
       "      <td>1.115640</td>\n",
       "      <td>0.802032</td>\n",
       "      <td>1.007847</td>\n",
       "      <td>0.895366</td>\n",
       "      <td>1.116979</td>\n",
       "      <td>0.345379</td>\n",
       "      <td>0.556281</td>\n",
       "      <td>-0.831309</td>\n",
       "      <td>1.041257</td>\n",
       "      <td>1.108273</td>\n",
       "      <td>1.117401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160354</td>\n",
       "      <td>249</td>\n",
       "      <td>0.122096</td>\n",
       "      <td>-0.436362</td>\n",
       "      <td>-0.433270</td>\n",
       "      <td>0.187601</td>\n",
       "      <td>0.417384</td>\n",
       "      <td>0.398560</td>\n",
       "      <td>0.257619</td>\n",
       "      <td>0.321783</td>\n",
       "      <td>0.434015</td>\n",
       "      <td>0.545152</td>\n",
       "      <td>0.251961</td>\n",
       "      <td>0.351586</td>\n",
       "      <td>1.199893</td>\n",
       "      <td>-0.798474</td>\n",
       "      <td>0.364304</td>\n",
       "      <td>0.270907</td>\n",
       "      <td>0.279069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160355</td>\n",
       "      <td>249</td>\n",
       "      <td>0.122096</td>\n",
       "      <td>-1.041215</td>\n",
       "      <td>-1.106176</td>\n",
       "      <td>1.084292</td>\n",
       "      <td>1.116010</td>\n",
       "      <td>1.216537</td>\n",
       "      <td>1.193683</td>\n",
       "      <td>0.804910</td>\n",
       "      <td>1.173252</td>\n",
       "      <td>1.117168</td>\n",
       "      <td>1.195119</td>\n",
       "      <td>0.349158</td>\n",
       "      <td>1.197165</td>\n",
       "      <td>-1.106885</td>\n",
       "      <td>1.137965</td>\n",
       "      <td>1.149328</td>\n",
       "      <td>1.161691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160356</td>\n",
       "      <td>249</td>\n",
       "      <td>0.122096</td>\n",
       "      <td>-1.041209</td>\n",
       "      <td>-1.110238</td>\n",
       "      <td>1.090413</td>\n",
       "      <td>1.190476</td>\n",
       "      <td>1.244343</td>\n",
       "      <td>1.202010</td>\n",
       "      <td>0.805191</td>\n",
       "      <td>1.198369</td>\n",
       "      <td>1.175537</td>\n",
       "      <td>1.197494</td>\n",
       "      <td>0.349337</td>\n",
       "      <td>1.302222</td>\n",
       "      <td>-1.066472</td>\n",
       "      <td>1.105729</td>\n",
       "      <td>1.183542</td>\n",
       "      <td>1.177643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160357</td>\n",
       "      <td>249</td>\n",
       "      <td>0.122096</td>\n",
       "      <td>1.076516</td>\n",
       "      <td>1.164376</td>\n",
       "      <td>-0.979110</td>\n",
       "      <td>-0.725583</td>\n",
       "      <td>-0.827415</td>\n",
       "      <td>-0.917688</td>\n",
       "      <td>-0.351408</td>\n",
       "      <td>-0.731107</td>\n",
       "      <td>-0.490898</td>\n",
       "      <td>-0.918857</td>\n",
       "      <td>0.352486</td>\n",
       "      <td>0.455069</td>\n",
       "      <td>-0.004316</td>\n",
       "      <td>-0.763951</td>\n",
       "      <td>-0.913721</td>\n",
       "      <td>-0.924903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160358</td>\n",
       "      <td>249</td>\n",
       "      <td>0.122096</td>\n",
       "      <td>1.499944</td>\n",
       "      <td>1.164376</td>\n",
       "      <td>-1.118475</td>\n",
       "      <td>-0.823235</td>\n",
       "      <td>-0.832771</td>\n",
       "      <td>-1.247665</td>\n",
       "      <td>-0.430467</td>\n",
       "      <td>-0.764017</td>\n",
       "      <td>-0.470468</td>\n",
       "      <td>-1.245302</td>\n",
       "      <td>0.350957</td>\n",
       "      <td>0.686889</td>\n",
       "      <td>0.087675</td>\n",
       "      <td>-0.892895</td>\n",
       "      <td>-1.307172</td>\n",
       "      <td>-1.301703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160359 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0         1 -0.701701 -1.041427 -1.111321  1.046626  1.037990  1.024534   \n",
       "1         1 -0.701701 -1.041270 -1.111050  1.054394  1.055929  1.043169   \n",
       "2         1 -0.701701 -1.041645 -1.109426  1.059103  1.023520  1.050946   \n",
       "3         1 -0.701701 -1.041342 -1.110238  1.059103  0.979517  1.033851   \n",
       "4         1 -0.701701 -1.041500 -1.110779  1.059573  0.980025  1.065766   \n",
       "...     ...       ...       ...       ...       ...       ...       ...   \n",
       "160354  249  0.122096 -0.436362 -0.433270  0.187601  0.417384  0.398560   \n",
       "160355  249  0.122096 -1.041215 -1.106176  1.084292  1.116010  1.216537   \n",
       "160356  249  0.122096 -1.041209 -1.110238  1.090413  1.190476  1.244343   \n",
       "160357  249  0.122096  1.076516  1.164376 -0.979110 -0.725583 -0.827415   \n",
       "160358  249  0.122096  1.499944  1.164376 -1.118475 -0.823235 -0.832771   \n",
       "\n",
       "               7         8         9        10        11        12        13  \\\n",
       "0       1.117707  0.802032  0.983932  0.950816  1.113752  0.345199  0.616065   \n",
       "1       1.114204  0.801891  0.978273  0.956653  1.117528  0.345649  0.527629   \n",
       "2       1.117133  0.802172  1.001948  0.892447  1.118380  0.345289  0.549211   \n",
       "3       1.118224  0.802383  0.992713  0.851589  1.121060  0.345739  0.556653   \n",
       "4       1.115640  0.802032  1.007847  0.895366  1.116979  0.345379  0.556281   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "160354  0.257619  0.321783  0.434015  0.545152  0.251961  0.351586  1.199893   \n",
       "160355  1.193683  0.804910  1.173252  1.117168  1.195119  0.349158  1.197165   \n",
       "160356  1.202010  0.805191  1.198369  1.175537  1.197494  0.349337  1.302222   \n",
       "160357 -0.917688 -0.351408 -0.731107 -0.490898 -0.918857  0.352486  0.455069   \n",
       "160358 -1.247665 -0.430467 -0.764017 -0.470468 -1.245302  0.350957  0.686889   \n",
       "\n",
       "              14        15        16        17  \n",
       "0      -0.844470  1.009021  1.121958  1.119482  \n",
       "1      -0.828118  1.009021  1.116826  1.120138  \n",
       "2      -0.846729  0.944549  1.112549  1.108820  \n",
       "3      -0.912666  1.009021  1.106562  1.113053  \n",
       "4      -0.831309  1.041257  1.108273  1.117401  \n",
       "...          ...       ...       ...       ...  \n",
       "160354 -0.798474  0.364304  0.270907  0.279069  \n",
       "160355 -1.106885  1.137965  1.149328  1.161691  \n",
       "160356 -1.066472  1.105729  1.183542  1.177643  \n",
       "160357 -0.004316 -0.763951 -0.913721 -0.924903  \n",
       "160358  0.087675 -0.892895 -1.307172 -1.301703  \n",
       "\n",
       "[160359 rows x 18 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = train_data2.shape[0] - 2\n",
    "temp_time = train_data2[1][i + 1]\n",
    "count = 1\n",
    "max_count = 1\n",
    "while (i != -1):\n",
    "    count += 1\n",
    "    if (train_data2[0][i] == train_data2[0][i + 1]):\n",
    "        train_data2[1][i] = temp_time\n",
    "    else:\n",
    "        temp_time = train_data2[1][i]\n",
    "        if (count > max_count):\n",
    "            max_count = count\n",
    "            count = 1\n",
    "    i -= 1\n",
    "train_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.701701</td>\n",
       "      <td>-1.041427</td>\n",
       "      <td>-1.111321</td>\n",
       "      <td>1.046626</td>\n",
       "      <td>1.037990</td>\n",
       "      <td>1.024534</td>\n",
       "      <td>1.117707</td>\n",
       "      <td>0.802032</td>\n",
       "      <td>0.983932</td>\n",
       "      <td>0.950816</td>\n",
       "      <td>1.113752</td>\n",
       "      <td>0.345199</td>\n",
       "      <td>0.616065</td>\n",
       "      <td>-0.844470</td>\n",
       "      <td>1.009021</td>\n",
       "      <td>1.121958</td>\n",
       "      <td>1.119482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.701701</td>\n",
       "      <td>-1.041270</td>\n",
       "      <td>-1.111050</td>\n",
       "      <td>1.054394</td>\n",
       "      <td>1.055929</td>\n",
       "      <td>1.043169</td>\n",
       "      <td>1.114204</td>\n",
       "      <td>0.801891</td>\n",
       "      <td>0.978273</td>\n",
       "      <td>0.956653</td>\n",
       "      <td>1.117528</td>\n",
       "      <td>0.345649</td>\n",
       "      <td>0.527629</td>\n",
       "      <td>-0.828118</td>\n",
       "      <td>1.009021</td>\n",
       "      <td>1.116826</td>\n",
       "      <td>1.120138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.701701</td>\n",
       "      <td>-1.041645</td>\n",
       "      <td>-1.109426</td>\n",
       "      <td>1.059103</td>\n",
       "      <td>1.023520</td>\n",
       "      <td>1.050946</td>\n",
       "      <td>1.117133</td>\n",
       "      <td>0.802172</td>\n",
       "      <td>1.001948</td>\n",
       "      <td>0.892447</td>\n",
       "      <td>1.118380</td>\n",
       "      <td>0.345289</td>\n",
       "      <td>0.549211</td>\n",
       "      <td>-0.846729</td>\n",
       "      <td>0.944549</td>\n",
       "      <td>1.112549</td>\n",
       "      <td>1.108820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.701701</td>\n",
       "      <td>-1.041342</td>\n",
       "      <td>-1.110238</td>\n",
       "      <td>1.059103</td>\n",
       "      <td>0.979517</td>\n",
       "      <td>1.033851</td>\n",
       "      <td>1.118224</td>\n",
       "      <td>0.802383</td>\n",
       "      <td>0.992713</td>\n",
       "      <td>0.851589</td>\n",
       "      <td>1.121060</td>\n",
       "      <td>0.345739</td>\n",
       "      <td>0.556653</td>\n",
       "      <td>-0.912666</td>\n",
       "      <td>1.009021</td>\n",
       "      <td>1.106562</td>\n",
       "      <td>1.113053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.701701</td>\n",
       "      <td>-1.041500</td>\n",
       "      <td>-1.110779</td>\n",
       "      <td>1.059573</td>\n",
       "      <td>0.980025</td>\n",
       "      <td>1.065766</td>\n",
       "      <td>1.115640</td>\n",
       "      <td>0.802032</td>\n",
       "      <td>1.007847</td>\n",
       "      <td>0.895366</td>\n",
       "      <td>1.116979</td>\n",
       "      <td>0.345379</td>\n",
       "      <td>0.556281</td>\n",
       "      <td>-0.831309</td>\n",
       "      <td>1.041257</td>\n",
       "      <td>1.108273</td>\n",
       "      <td>1.117401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160354</td>\n",
       "      <td>249</td>\n",
       "      <td>0.122096</td>\n",
       "      <td>-0.436362</td>\n",
       "      <td>-0.433270</td>\n",
       "      <td>0.187601</td>\n",
       "      <td>0.417384</td>\n",
       "      <td>0.398560</td>\n",
       "      <td>0.257619</td>\n",
       "      <td>0.321783</td>\n",
       "      <td>0.434015</td>\n",
       "      <td>0.545152</td>\n",
       "      <td>0.251961</td>\n",
       "      <td>0.351586</td>\n",
       "      <td>1.199893</td>\n",
       "      <td>-0.798474</td>\n",
       "      <td>0.364304</td>\n",
       "      <td>0.270907</td>\n",
       "      <td>0.279069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160355</td>\n",
       "      <td>249</td>\n",
       "      <td>0.122096</td>\n",
       "      <td>-1.041215</td>\n",
       "      <td>-1.106176</td>\n",
       "      <td>1.084292</td>\n",
       "      <td>1.116010</td>\n",
       "      <td>1.216537</td>\n",
       "      <td>1.193683</td>\n",
       "      <td>0.804910</td>\n",
       "      <td>1.173252</td>\n",
       "      <td>1.117168</td>\n",
       "      <td>1.195119</td>\n",
       "      <td>0.349158</td>\n",
       "      <td>1.197165</td>\n",
       "      <td>-1.106885</td>\n",
       "      <td>1.137965</td>\n",
       "      <td>1.149328</td>\n",
       "      <td>1.161691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160356</td>\n",
       "      <td>249</td>\n",
       "      <td>0.122096</td>\n",
       "      <td>-1.041209</td>\n",
       "      <td>-1.110238</td>\n",
       "      <td>1.090413</td>\n",
       "      <td>1.190476</td>\n",
       "      <td>1.244343</td>\n",
       "      <td>1.202010</td>\n",
       "      <td>0.805191</td>\n",
       "      <td>1.198369</td>\n",
       "      <td>1.175537</td>\n",
       "      <td>1.197494</td>\n",
       "      <td>0.349337</td>\n",
       "      <td>1.302222</td>\n",
       "      <td>-1.066472</td>\n",
       "      <td>1.105729</td>\n",
       "      <td>1.183542</td>\n",
       "      <td>1.177643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160357</td>\n",
       "      <td>249</td>\n",
       "      <td>0.122096</td>\n",
       "      <td>1.076516</td>\n",
       "      <td>1.164376</td>\n",
       "      <td>-0.979110</td>\n",
       "      <td>-0.725583</td>\n",
       "      <td>-0.827415</td>\n",
       "      <td>-0.917688</td>\n",
       "      <td>-0.351408</td>\n",
       "      <td>-0.731107</td>\n",
       "      <td>-0.490898</td>\n",
       "      <td>-0.918857</td>\n",
       "      <td>0.352486</td>\n",
       "      <td>0.455069</td>\n",
       "      <td>-0.004316</td>\n",
       "      <td>-0.763951</td>\n",
       "      <td>-0.913721</td>\n",
       "      <td>-0.924903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160358</td>\n",
       "      <td>249</td>\n",
       "      <td>0.122096</td>\n",
       "      <td>1.499944</td>\n",
       "      <td>1.164376</td>\n",
       "      <td>-1.118475</td>\n",
       "      <td>-0.823235</td>\n",
       "      <td>-0.832771</td>\n",
       "      <td>-1.247665</td>\n",
       "      <td>-0.430467</td>\n",
       "      <td>-0.764017</td>\n",
       "      <td>-0.470468</td>\n",
       "      <td>-1.245302</td>\n",
       "      <td>0.350957</td>\n",
       "      <td>0.686889</td>\n",
       "      <td>0.087675</td>\n",
       "      <td>-0.892895</td>\n",
       "      <td>-1.307172</td>\n",
       "      <td>-1.301703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160359 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0         1 -0.701701 -1.041427 -1.111321  1.046626  1.037990  1.024534   \n",
       "1         1 -0.701701 -1.041270 -1.111050  1.054394  1.055929  1.043169   \n",
       "2         1 -0.701701 -1.041645 -1.109426  1.059103  1.023520  1.050946   \n",
       "3         1 -0.701701 -1.041342 -1.110238  1.059103  0.979517  1.033851   \n",
       "4         1 -0.701701 -1.041500 -1.110779  1.059573  0.980025  1.065766   \n",
       "...     ...       ...       ...       ...       ...       ...       ...   \n",
       "160354  249  0.122096 -0.436362 -0.433270  0.187601  0.417384  0.398560   \n",
       "160355  249  0.122096 -1.041215 -1.106176  1.084292  1.116010  1.216537   \n",
       "160356  249  0.122096 -1.041209 -1.110238  1.090413  1.190476  1.244343   \n",
       "160357  249  0.122096  1.076516  1.164376 -0.979110 -0.725583 -0.827415   \n",
       "160358  249  0.122096  1.499944  1.164376 -1.118475 -0.823235 -0.832771   \n",
       "\n",
       "               7         8         9        10        11        12        13  \\\n",
       "0       1.117707  0.802032  0.983932  0.950816  1.113752  0.345199  0.616065   \n",
       "1       1.114204  0.801891  0.978273  0.956653  1.117528  0.345649  0.527629   \n",
       "2       1.117133  0.802172  1.001948  0.892447  1.118380  0.345289  0.549211   \n",
       "3       1.118224  0.802383  0.992713  0.851589  1.121060  0.345739  0.556653   \n",
       "4       1.115640  0.802032  1.007847  0.895366  1.116979  0.345379  0.556281   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "160354  0.257619  0.321783  0.434015  0.545152  0.251961  0.351586  1.199893   \n",
       "160355  1.193683  0.804910  1.173252  1.117168  1.195119  0.349158  1.197165   \n",
       "160356  1.202010  0.805191  1.198369  1.175537  1.197494  0.349337  1.302222   \n",
       "160357 -0.917688 -0.351408 -0.731107 -0.490898 -0.918857  0.352486  0.455069   \n",
       "160358 -1.247665 -0.430467 -0.764017 -0.470468 -1.245302  0.350957  0.686889   \n",
       "\n",
       "              14        15        16        17  \n",
       "0      -0.844470  1.009021  1.121958  1.119482  \n",
       "1      -0.828118  1.009021  1.116826  1.120138  \n",
       "2      -0.846729  0.944549  1.112549  1.108820  \n",
       "3      -0.912666  1.009021  1.106562  1.113053  \n",
       "4      -0.831309  1.041257  1.108273  1.117401  \n",
       "...          ...       ...       ...       ...  \n",
       "160354 -0.798474  0.364304  0.270907  0.279069  \n",
       "160355 -1.106885  1.137965  1.149328  1.161691  \n",
       "160356 -1.066472  1.105729  1.183542  1.177643  \n",
       "160357 -0.004316 -0.763951 -0.913721 -0.924903  \n",
       "160358  0.087675 -0.892895 -1.307172 -1.301703  \n",
       "\n",
       "[160359 rows x 18 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = 0.001\n",
    "for i in range(1,train_data2.shape[1]):\n",
    "    m = np.mean(train_data2[i])\n",
    "    v = np.var(train_data2[i])\n",
    "    train_data2[i] = (train_data2[i] - m)/np.sqrt(v + eps)\n",
    "train_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = train_data2.loc[:,2:]\n",
    "x_train = np.array(temp_data, dtype = np.float32())\n",
    "temp_data = train_data2.loc[:,1]\n",
    "y_train = np.array(temp_data, dtype = np.float32())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.0414271 , -1.1113211 ,  1.0466256 , ...,  1.0090212 ,\n",
       "         1.121958  ,  1.1194824 ],\n",
       "       [-1.0412698 , -1.1110502 ,  1.0543942 , ...,  1.0090212 ,\n",
       "         1.116826  ,  1.1201382 ],\n",
       "       [-1.0416449 , -1.1094255 ,  1.0591025 , ...,  0.94454944,\n",
       "         1.1125494 ,  1.1088197 ],\n",
       "       ...,\n",
       "       [-1.0412093 , -1.1102378 ,  1.0904126 , ...,  1.1057287 ,\n",
       "         1.1835415 ,  1.1776426 ],\n",
       "       [ 1.0765164 ,  1.1643755 , -0.97911   , ..., -0.7639513 ,\n",
       "        -0.91372144, -0.92490274],\n",
       "       [ 1.4999441 ,  1.1643755 , -1.118475  , ..., -0.89289474,\n",
       "        -1.3071721 , -1.3017033 ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "handle = open('./CMAPSSData/RUL_FD.txt', \"r\")\n",
    "a = list()\n",
    "for line in handle:\n",
    "    a.append(np.float32(line))\n",
    "handle.close()\n",
    "y_test = np.array(a)\n",
    "y_test\n",
    "\n",
    "test_data = pd.read_csv('./CMAPSSData/test_FD.txt', sep = ' ', header=None)\n",
    "mas = [4, 5, 9, 10, 14, 20, 22, 23, 26, 27]\n",
    "test_data2 = test_data.drop(test_data.columns[mas], axis=1)\n",
    "test_data2.columns = range(18)\n",
    "\n",
    "i = 1\n",
    "j = 0\n",
    "test_data2[1][0] = y_test[j]\n",
    "\n",
    "while (i != test_data2.shape[0]):\n",
    "    if (test_data2[0][i] == test_data2[0][i - 1]):\n",
    "        test_data2[1][i] = y_test[j]\n",
    "    else:\n",
    "        j += 1\n",
    "        test_data2[1][i] = y_test[j]\n",
    "    i += 1\n",
    "\n",
    "\n",
    "eps = 0.001\n",
    "for i in range(1,test_data2.shape[1]):\n",
    "    m = np.mean(test_data2[i])\n",
    "    v = np.var(test_data2[i])\n",
    "    test_data2[i] = (test_data2[i] - m)/np.sqrt(v + eps)\n",
    "\n",
    "eps = 0.001\n",
    "m = np.mean(y_test)\n",
    "v = np.var(y_test)\n",
    "y_test = (y_test - m)/np.sqrt(v + eps)\n",
    "\n",
    "temp_data = test_data2.loc[:,2:]\n",
    "x_test = np.array(temp_data, dtype = np.float32())\n",
    "temp_data = test_data2.loc[:,1]\n",
    "y_test = np.array(temp_data, dtype = np.float32())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_104 (LSTM)              (None, 200)               161600    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 50)                10050     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 171,701\n",
      "Trainable params: 171,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "l2_lambda = 0.0001\n",
    "model_lstm2 = Sequential()\n",
    "#model_lstm2.add(Embedding(x_train.shape[0], 16, input_length=16))\n",
    "model_lstm2.add(InputLayer(input_shape=[x_train.shape[1], 1]))\n",
    "x_train2 = np.reshape(x_train, [x_train.shape[0], x_train.shape[1], 1])\n",
    "y_train2 = np.reshape(y_train, [y_train.shape[0], 1])\n",
    "x_test2 = np.reshape(x_test, [x_test.shape[0], x_test.shape[1], 1])\n",
    "y_test2 = np.reshape(y_test, [y_test.shape[0], 1])\n",
    "model_lstm2.add(LSTM(200, kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "#model_lstm2.add(LSTM(50, dropout=0.5, recurrent_dropout=0.2, kernel_regularizer=regularizers.l2(l2_lambda), return_sequences=True))\n",
    "#model_lstm2.add(LSTM(20, dropout=0.5, recurrent_dropout=0.2, kernel_regularizer=regularizers.l2(l2_lambda)))\n",
    "#model_lstm.add(Dropout(0.9))\n",
    "model_lstm2.add(Dropout(0.3))\n",
    "model_lstm2.add(Dense(50, activation='tanh'))\n",
    "model_lstm2.add(Dropout(0.5))\n",
    "#model_lstm2.add(Dense(5, activation='tanh'))\n",
    "#model_lstm2.add(Dropout(0.5))\n",
    "model_lstm2.add(Dense(1, activation='relu'))\n",
    "model_lstm2.compile(optimizer='rmsprop', loss='mse', metrics=['accuracy'])\n",
    "model_lstm2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160359 samples, validate on 104897 samples\n",
      "Epoch 1/50\n",
      "160359/160359 [==============================] - 286s 2ms/sample - loss: 1.0001 - accuracy: 0.0000e+00 - val_loss: 1.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "160359/160359 [==============================] - 235s 1ms/sample - loss: 1.0000 - accuracy: 0.0000e+00 - val_loss: 1.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "160359/160359 [==============================] - 253s 2ms/sample - loss: 1.0000 - accuracy: 0.0000e+00 - val_loss: 1.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      " 32800/160359 [=====>........................] - ETA: 2:50 - loss: 1.0101 - accuracy: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-4f8cdd3f54b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory_lstm2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_lstm2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_test2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    485\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history_lstm2 = model_lstm2.fit(x_train2, y_train2, epochs=50, batch_size=20, validation_data = (x_test2, y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104897,)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test2\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1678352 into shape (1,1,104897)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-3b52f7148219>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_test2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mx_test2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    290\u001b[0m            [5, 6]])\n\u001b[0;32m    291\u001b[0m     \"\"\"\n\u001b[1;32m--> 292\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reshape'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 1678352 into shape (1,1,104897)"
     ]
    }
   ],
   "source": [
    "x_test2 = np.reshape(x_test2, [1, x_test2.shape[0], x_test2.shape[1]])\n",
    "x_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.0430378 , -1.1113998 ,  1.0809845 , ...,  1.0316682 ,\n",
       "          1.1033361 ,  1.1114497 ],\n",
       "        [-1.0433402 , -1.1130239 ,  1.0501105 , ...,  1.0639969 ,\n",
       "          1.1170101 ,  1.1140277 ],\n",
       "        [-1.0431588 , -1.1119412 ,  1.0677865 , ...,  1.0639969 ,\n",
       "          1.1221379 ,  1.1175888 ],\n",
       "        ...,\n",
       "        [ 1.0741011 ,  1.1650683 , -0.97178715, ..., -0.7787352 ,\n",
       "         -0.9622999 , -0.9588211 ],\n",
       "        [ 0.16690108,  0.7825892 ,  0.2601115 , ...,  0.19112377,\n",
       "         -0.12903756, -0.12559074],\n",
       "        [ 1.0746274 ,  1.1620908 , -0.9604745 , ..., -0.8110638 ,\n",
       "         -0.959736  , -0.95721155]]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones([3,3])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 9 into shape (5,5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-176c834b6acb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 9 into shape (5,5)"
     ]
    }
   ],
   "source": [
    "a.reshape(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
