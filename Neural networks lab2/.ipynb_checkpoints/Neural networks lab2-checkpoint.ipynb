{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images belonging to 10 classes.\n",
      "Found 30 images belonging to 10 classes.\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 36s 12s/step - loss: 2.6056 - accuracy: 0.2571 - val_loss: 1.1321 - val_accuracy: 0.8000\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 42s 14s/step - loss: 0.7849 - accuracy: 0.8222 - val_loss: 0.3922 - val_accuracy: 0.9000\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 25s 8s/step - loss: 0.4649 - accuracy: 0.8200 - val_loss: 0.2416 - val_accuracy: 0.8667\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 41s 14s/step - loss: 0.1481 - accuracy: 0.9222 - val_loss: 0.1726 - val_accuracy: 0.9333\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 32s 11s/step - loss: 0.0423 - accuracy: 0.9857 - val_loss: 0.0396 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 33s 11s/step - loss: 0.0331 - accuracy: 0.9714 - val_loss: 0.0829 - val_accuracy: 0.9667\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 41s 14s/step - loss: 0.0436 - accuracy: 0.9889 - val_loss: 0.0811 - val_accuracy: 0.9667\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 33s 11s/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0313 - val_accuracy: 0.9667\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 41s 14s/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0149 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 25s 8s/step - loss: 0.0646 - accuracy: 0.9800 - val_loss: 0.0149 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x115ed4988>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "train_generator=train_datagen.flow_from_directory('./Train/', target_size=(224,224), color_mode='rgb', batch_size=30, class_mode='categorical', shuffle=True)\n",
    "\n",
    "valid_datagen=ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "valid_generator=valid_datagen.flow_from_directory('./Valid/', target_size=(224,224), color_mode='rgb', batch_size=1, class_mode='categorical', shuffle=True)\n",
    "\n",
    "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "base_model=MobileNet(weights='imagenet',include_top=False)\n",
    "output_model=base_model.output\n",
    "output_model=GlobalAveragePooling2D()(output_model)\n",
    "output_model=Dense(1024,activation='relu')(output_model)\n",
    "output_model=Dense(512,activation='relu')(output_model)\n",
    "numClasses = 10\n",
    "output_model=Dense(numClasses,activation='softmax')(output_model)\n",
    "model=Model(inputs=base_model.input,outputs=output_model)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable=False\n",
    "    \n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "step_size_train=train_generator.n//train_generator.batch_size\n",
    "step_size_valid=valid_generator.n//valid_generator.batch_size\n",
    "model.fit_generator(generator=train_generator, steps_per_epoch=step_size_train, validation_data=valid_generator, validation_steps =step_size_valid, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('our_first_convmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('our_first_convmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def answer(y):\n",
    "    a = max(y[0])\n",
    "    for i in range(len(y[0])):\n",
    "        if (a == y[0][i]):\n",
    "            j = i\n",
    "            break\n",
    "    if (j == 0):\n",
    "        print('Это гора')\n",
    "    elif (j == 1):\n",
    "        print('Это котик')\n",
    "    elif (j == 2):\n",
    "        print('Это цветы')\n",
    "    elif (j == 3):\n",
    "        print('Это дерево')\n",
    "    elif (j == 4):\n",
    "        print('Это звёзды')\n",
    "    elif (j == 5):\n",
    "        print('Это камни')\n",
    "    elif (j == 6):\n",
    "        print('Это ковёр')\n",
    "    elif (j == 7):\n",
    "        print('Это машина')\n",
    "    elif (j == 8):\n",
    "        print('Это самолёт')\n",
    "    elif (j == 9):\n",
    "        print('Это хомяк')\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 images belonging to 1 classes.\n",
      "Это хомяк\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "valid_datagen=ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "valid_generator=valid_datagen.flow_from_directory('./Proverka/', target_size=(224,224), color_mode='rgb', batch_size=1, class_mode='categorical', shuffle=True)\n",
    "y = model.predict(valid_generator)\n",
    "answer(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "# Нормируем данные и приводим массивы к виду (N,size1,size1,1)\n",
    "image_size = x_train.shape[1]\n",
    "x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
    "x_test = np.reshape(x_test, [-1, image_size, image_size, 1])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(loc=0.5, scale=0.5, size=x_train.shape)\n",
    "x_train_noisy = x_train + noise\n",
    "noise = np.random.normal(loc=0.5, scale=0.5, size=x_test.shape)\n",
    "x_test_noisy = x_test + noise\n",
    "# Ограничим итоговые значения интервалом [0,1]\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (image_size, image_size, 1)\n",
    "batch_size = 128\n",
    "kernel_size = 3\n",
    "latent_dim = 16\n",
    "# Зададим два сверточных слоя и число нейронов в каждом слое:\n",
    "layer_filters = [32, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Activation, Dense, Input\n",
    "from tensorflow.keras.layers import Conv2D, Flatten\n",
    "from tensorflow.keras.layers import Reshape, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "latent_vector (Dense)        (None, 16)                50192     \n",
      "=================================================================\n",
      "Total params: 69,008\n",
      "Trainable params: 69,008\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = encoder_inputs\n",
    "# Стек из сверточных слоев (strides - дискрет сдвига окна свертки\n",
    "# в пикселях, padding - без заполнения нулями):\n",
    "for filters in layer_filters:\n",
    "    x = Conv2D(filters=filters, kernel_size=kernel_size, strides=2, activation='relu', padding='same')(x)\n",
    "\n",
    "# Запоминаем размерность выхода для построения модели декодера\n",
    "shape = K.int_shape(x)\n",
    "# Преобразуем многомерный массив в вектор\n",
    "x = Flatten()(x)\n",
    "latent = Dense(latent_dim, name='latent_vector')(x)\n",
    "# Итоговый код - одномерный вектор меньшей размерности (latent_dim)\n",
    "# Итоговая модель энкодера:\n",
    "encoder = Model(encoder_inputs , latent, name='encoder')\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3136)              53312     \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 28, 28, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 1)         289       \n",
      "_________________________________________________________________\n",
      "decoder_output (Activation)  (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 108,993\n",
      "Trainable params: 108,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 103s 2ms/sample - loss: 0.0640 - val_loss: 0.0349\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 94s 2ms/sample - loss: 0.0268 - val_loss: 0.0223\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 92s 2ms/sample - loss: 0.0213 - val_loss: 0.0197\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 95s 2ms/sample - loss: 0.0194 - val_loss: 0.0186\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 91s 2ms/sample - loss: 0.0185 - val_loss: 0.0179\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 97s 2ms/sample - loss: 0.0178 - val_loss: 0.0175\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 99s 2ms/sample - loss: 0.0173 - val_loss: 0.0172\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 92s 2ms/sample - loss: 0.0169 - val_loss: 0.0170\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 91s 2ms/sample - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 101s 2ms/sample - loss: 0.0164 - val_loss: 0.0166\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 93s 2ms/sample - loss: 0.0161 - val_loss: 0.0166\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 91s 2ms/sample - loss: 0.0159 - val_loss: 0.0163\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 92s 2ms/sample - loss: 0.0158 - val_loss: 0.0166\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 90s 1ms/sample - loss: 0.0156 - val_loss: 0.0164\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 94s 2ms/sample - loss: 0.0155 - val_loss: 0.0161\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 94s 2ms/sample - loss: 0.0153 - val_loss: 0.0160\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 99s 2ms/sample - loss: 0.0152 - val_loss: 0.0160\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 104s 2ms/sample - loss: 0.0151 - val_loss: 0.0158\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 97s 2ms/sample - loss: 0.0150 - val_loss: 0.0158\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 89s 1ms/sample - loss: 0.0149 - val_loss: 0.0157\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 92s 2ms/sample - loss: 0.0148 - val_loss: 0.0160\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 91s 2ms/sample - loss: 0.0147 - val_loss: 0.0157\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 87s 1ms/sample - loss: 0.0147 - val_loss: 0.0157\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 94s 2ms/sample - loss: 0.0146 - val_loss: 0.0156\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 95s 2ms/sample - loss: 0.0145 - val_loss: 0.0156\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 95s 2ms/sample - loss: 0.0145 - val_loss: 0.0155\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - 94s 2ms/sample - loss: 0.0144 - val_loss: 0.0156\n",
      "Epoch 28/30\n",
      "60000/60000 [==============================] - 94s 2ms/sample - loss: 0.0143 - val_loss: 0.0156\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 96s 2ms/sample - loss: 0.0143 - val_loss: 0.0155\n",
      "Epoch 30/30\n",
      "60000/60000 [==============================] - 99s 2ms/sample - loss: 0.0142 - val_loss: 0.0154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17f00148>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n",
    "# Обратное преобразование к размеру \"shape\":\n",
    "x = Dense(shape[1] * shape[2] * shape[3])(latent_inputs)\n",
    "# Выход должен быть трехмерным массивом:\n",
    "x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "# Вместо сверточных слоев \"разверточные\", цикл в обратном порядке:\n",
    "for filters in layer_filters[::-1]:\n",
    "    x = Conv2DTranspose(filters=filters, kernel_size=kernel_size, strides=2, activation='relu', padding='same')(x)\n",
    "\n",
    "x = Conv2DTranspose(filters=1, kernel_size=kernel_size, padding='same')(x)\n",
    "outputs = Activation('sigmoid', name='decoder_output')(x)\n",
    "# Итоговая модель декодера:\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "#Объединив две модели в одну мы получим наш автоэнкодер:\n",
    "autoencoder = Model(encoder_inputs, decoder(encoder(encoder_inputs)), name='autoencoder')\n",
    "autoencoder.compile(loss='mse', optimizer='adam')\n",
    "# Запускаем модель на обучение, используя незашумленные данные как эталонные значения.\n",
    "autoencoder.fit(x_train_noisy, x_train, validation_data=(x_test_noisy, x_test), epochs=30, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autoencoder.save('autoencoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28, 1)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'PIL.ImageDraw' has no attribute 'draw'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-2a84b979a47a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mImageDraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'PIL.ImageDraw' has no attribute 'draw'"
     ]
    }
   ],
   "source": [
    "\n",
    "y = autoencoder.predict(x_test_noisy)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\PIL\\ImageDraw.py\u001b[0m in \u001b[0;36mDraw\u001b[1;34m(im, mode)\u001b[0m\n\u001b[0;32m    465\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    467\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'getdraw'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-4e6808e50cda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImageDraw\u001b[0m \u001b[1;31m#Подключим необходимые библиотеки.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdraw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Создаем инструмент для рисования.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mdraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\PIL\\ImageDraw.py\u001b[0m in \u001b[0;36mDraw\u001b[1;34m(im, mode)\u001b[0m\n\u001b[0;32m    466\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\PIL\\ImageDraw.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, im, mode)\u001b[0m\n\u001b[0;32m     58\u001b[0m            \u001b[0mdefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \"\"\"\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadonly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# make it writeable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'load'"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw #Подключим необходимые библиотеки. \n",
    "draw = ImageDraw.Draw(y) #Создаем инструмент для рисования.\n",
    "for i in range(28):\n",
    "    for j in range(28):\n",
    "        draw.point((j,i),y[1,i][j][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
